# Model hyperparameters
learning_rate: 0.002 # The learning rate for the optimizer
batch_size: 3        # Number of samples per batch
test_batch_size: 8        # Number of samples per batch
epochs: 1            # Total training epochs
#optimizer: "ranger"       # Optimization algorithm
dropout: 0.1     # Dropout regularization rate
weight_decay: 0.0001
k: 5 #not used
ninp: 384
nlayers: 48
nclass: 10
ntoken: 6 #AUGC + padding/N token
nhead: 12
#use_bpp: False
use_flip_aug: false
#bpp_file_folder: "../../input/bpp_files/"
gradient_accumulation_steps: 1
use_triangular_attention: false
pairwise_dimension: 128
dim_msa: 32
clip_grad_norm: 1
max_len: 177
log_interval: 2000 #save checkpoints every log_interval steps
previous_model_path: "../../exps/test41_biglr/models/epoch_14/pytorch_model_fsdp.bin"
use_noise_aug: false

#Data scaling
use_data_percentage: 1
use_dirty_data: true # turn off for data scaling and data dropout experiments


# Other configurations
fold: 0
nfolds: 6
input_dir: "../../input/"
gpu_id: "0"